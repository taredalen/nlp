{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHnEwgLcu4wj"
   },
   "source": [
    "# <center> TP - Text-Mining <center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEyABl2ou4wp"
   },
   "source": [
    "** Before you begin this notebook, please make sure that you have the data folder + that you are running this notebook from your workspace **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thnWx-pju4wp"
   },
   "source": [
    "_Now let's practice and use some text-mining techniques!_\n",
    "<br/>\n",
    "<br/> In this notebook we will study a mail dataset.\n",
    "Our final goal is to have some insights about the different subjects that appear in mailboxes. Among other problems, one big issue is that these mailboxes are poluted with a lot of spams.\n",
    "\n",
    "> The sequence we propose :\n",
    "- First you'll work on pre-processing the content of the mails\n",
    "- Then you will try to detect whether a mail is a spam or not through a supervized learning algorithm\n",
    "- Finally, once you've trained an algorithm to detect spams, you will try to identify the main topics in the remaining non-spam mails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUgRDEE-u4wp"
   },
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sr9Tx-Cu4wq"
   },
   "source": [
    "Data are separated in six parts, each containing around 5000 mails.\n",
    "<br/> Each of these data chunks are separated in two parts : \n",
    ">- hams : that is to say non-spam mails\n",
    "- spams : containing advertisement or unrelevant content\n",
    "\n",
    "\n",
    "get the zip file here :\n",
    "- https://drive.google.com/file/d/1j1xO3HSevP__ZsP2FvTx7p0UHJO__7rj/view?usp=sharing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGm7OI0Xu4wq"
   },
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KCGtBSmu4wq"
   },
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jl2w5Sytu4wq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c1679fed-895a-4531-8f10-dbe8ed12cf10"
   },
   "source": [
    "#Import usefull package\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "#Import nltk packages to manipulate text\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from nltk import word_tokenize, WordNetLemmatizer, PorterStemmer\n",
    "from nltk import NaiveBayesClassifier, classify\n",
    "from nltk import pos_tag\n",
    "from nltk import ngrams\n",
    "\n",
    "#***\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "# Let's add a path containing some useful nltk data\n",
    "nltk.data.path += ['/mnt/share/nltk_data']\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sebila/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sebila/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sebila/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sebila/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "IGS0r7ADu4wr"
   },
   "source": [
    "# Modify the variable hereunder to you repository\n",
    "path_data  = \"./enron1\""
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAwi-13Yu4wr"
   },
   "source": [
    "# I. Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "tkcrs9X3u4ws"
   },
   "source": [
    "# Nothing to understand\n",
    "def read_mails(folder):\n",
    "    \"\"\"\n",
    "    Reads all the mails contained in a folder and gathers them in a list\n",
    "    \n",
    "    Args :\n",
    "        folder (str) : path to the folder containing mails\n",
    "        \n",
    "    Returns :\n",
    "        mails_list (list) : a list containing mails contents\n",
    "    \"\"\"\n",
    "    \n",
    "    mails_list = []\n",
    "    files_list = os.listdir(folder)\n",
    "    for file_name in files_list:\n",
    "        file_content = open(folder + file_name, 'r', encoding='latin1')\n",
    "        mails_list.append(file_content.read())\n",
    "    file_content.close()\n",
    "    return mails_list"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRmfcmhau4ws"
   },
   "source": [
    "## A. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "['spam', '.DS_Store', 'ham', 'Summary.txt', '.idea']"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " os.listdir(path_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOXef6eBu4ws"
   },
   "source": [
    "Data are separated in six parts, each containing around 5000 mails.\n",
    "<br/> Each of these data chunks are separated in two parts : ham and spam\n",
    "<br/>Let's load the first data chunk"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "gKLtlTqyu4ws"
   },
   "source": [
    "spams = []\n",
    "hams = []\n",
    "for folder in os.listdir(path_data):\n",
    "    if not folder.startswith('.') and folder != 'Summary.txt' :\n",
    "        # print(path_data +  '/' + folder )\n",
    "        # Load corresponding spams\n",
    "        spams.extend(read_mails(path_data +  '/' + folder + '/'))\n",
    "        # Load corresponding hams\n",
    "        hams.extend(read_mails(path_data +  '/' + folder +'/'))"
   ],
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgAlfQ_Zu4wt"
   },
   "source": [
    "# II. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O01QenXHu4wu"
   },
   "source": [
    "## A. Preprocessing one email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASh8oqisu4wu"
   },
   "source": [
    "Display one email content"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "GsrgaUKju4wu"
   },
   "source": [
    "#Get one spam-email & print it\n",
    "single_email = spams[0]"
   ],
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xg48cXH9xnCa",
    "outputId": "c3bd2bf0-69c3-485c-dd89-63feab86bc09"
   },
   "source": [
    "print(single_email)"
   ],
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: what up , , your cam babe\n",
      "what are you looking for ?\n",
      "if your looking for a companion for friendship , love , a date , or just good ole '\n",
      "fashioned * * * * * * , then try our brand new site ; it was developed and created\n",
      "to help anyone find what they ' re looking for . a quick bio form and you ' re\n",
      "on the road to satisfaction in every sense of the word . . . . no matter what\n",
      "that may be !\n",
      "try it out and youll be amazed .\n",
      "have a terrific time this evening\n",
      "copy and pa ste the add . ress you see on the line below into your browser to come to the site .\n",
      "http : / / www . meganbang . biz / bld / acc /\n",
      "no more plz\n",
      "http : / / www . naturalgolden . com / retract /\n",
      "counterattack aitken step preemptive shoehorn scaup . electrocardiograph movie honeycomb . monster war brandywine pietism byrne catatonia . encomia lookup intervenor skeleton turn catfish .\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rex_JwA1u4wu"
   },
   "source": [
    "### Lower verbatim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDlT0RNlu4wu"
   },
   "source": [
    "Lower the content of the previously displayed mail"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2Qye7vA8u4wu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1117d34e-50a8-418f-877c-7d6876268ad3"
   },
   "source": [
    "#TODO : lower case the email & print it\n",
    "\n",
    "lower_mail = single_email.lower()\n",
    "print(lower_mail)"
   ],
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject: what up , , your cam babe\n",
      "what are you looking for ?\n",
      "if your looking for a companion for friendship , love , a date , or just good ole '\n",
      "fashioned * * * * * * , then try our brand new site ; it was developed and created\n",
      "to help anyone find what they ' re looking for . a quick bio form and you ' re\n",
      "on the road to satisfaction in every sense of the word . . . . no matter what\n",
      "that may be !\n",
      "try it out and youll be amazed .\n",
      "have a terrific time this evening\n",
      "copy and pa ste the add . ress you see on the line below into your browser to come to the site .\n",
      "http : / / www . meganbang . biz / bld / acc /\n",
      "no more plz\n",
      "http : / / www . naturalgolden . com / retract /\n",
      "counterattack aitken step preemptive shoehorn scaup . electrocardiograph movie honeycomb . monster war brandywine pietism byrne catatonia . encomia lookup intervenor skeleton turn catfish .\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf_2BNWzu4wu"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6mbksrDu4wv"
   },
   "source": [
    "Here we are using a word tokenizer to divide the sentence into tokens"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "YipGifXCu4wv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0edca7a3-f41f-4baa-aa48-8c5f62d24edb"
   },
   "source": [
    "#TODO : tokenize your email and print it\n",
    "tokenized_mail = nltk.word_tokenize(single_email)\n",
    "print(tokenized_mail)"
   ],
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject', ':', 'what', 'up', ',', ',', 'your', 'cam', 'babe', 'what', 'are', 'you', 'looking', 'for', '?', 'if', 'your', 'looking', 'for', 'a', 'companion', 'for', 'friendship', ',', 'love', ',', 'a', 'date', ',', 'or', 'just', 'good', 'ole', \"'\", 'fashioned', '*', '*', '*', '*', '*', '*', ',', 'then', 'try', 'our', 'brand', 'new', 'site', ';', 'it', 'was', 'developed', 'and', 'created', 'to', 'help', 'anyone', 'find', 'what', 'they', \"'\", 're', 'looking', 'for', '.', 'a', 'quick', 'bio', 'form', 'and', 'you', \"'\", 're', 'on', 'the', 'road', 'to', 'satisfaction', 'in', 'every', 'sense', 'of', 'the', 'word', '.', '.', '.', '.', 'no', 'matter', 'what', 'that', 'may', 'be', '!', 'try', 'it', 'out', 'and', 'youll', 'be', 'amazed', '.', 'have', 'a', 'terrific', 'time', 'this', 'evening', 'copy', 'and', 'pa', 'ste', 'the', 'add', '.', 'ress', 'you', 'see', 'on', 'the', 'line', 'below', 'into', 'your', 'browser', 'to', 'come', 'to', 'the', 'site', '.', 'http', ':', '/', '/', 'www', '.', 'meganbang', '.', 'biz', '/', 'bld', '/', 'acc', '/', 'no', 'more', 'plz', 'http', ':', '/', '/', 'www', '.', 'naturalgolden', '.', 'com', '/', 'retract', '/', 'counterattack', 'aitken', 'step', 'preemptive', 'shoehorn', 'scaup', '.', 'electrocardiograph', 'movie', 'honeycomb', '.', 'monster', 'war', 'brandywine', 'pietism', 'byrne', 'catatonia', '.', 'encomia', 'lookup', 'intervenor', 'skeleton', 'turn', 'catfish', '.']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eYWnd6xJu4wv",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "outputId": "17c46145-baa5-4230-e297-04d299b28bb1"
   },
   "source": [
    "#TODO : create bigrams of your email & print it\n",
    "bigrams = [bigram for bigram in ngrams(tokenized_mail, 2)]\n",
    "\n",
    "bigrams\n",
    "\n",
    "print(bigrams)"
   ],
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Subject', ':'), (':', 'what'), ('what', 'up'), ('up', ','), (',', ','), (',', 'your'), ('your', 'cam'), ('cam', 'babe'), ('babe', 'what'), ('what', 'are'), ('are', 'you'), ('you', 'looking'), ('looking', 'for'), ('for', '?'), ('?', 'if'), ('if', 'your'), ('your', 'looking'), ('looking', 'for'), ('for', 'a'), ('a', 'companion'), ('companion', 'for'), ('for', 'friendship'), ('friendship', ','), (',', 'love'), ('love', ','), (',', 'a'), ('a', 'date'), ('date', ','), (',', 'or'), ('or', 'just'), ('just', 'good'), ('good', 'ole'), ('ole', \"'\"), (\"'\", 'fashioned'), ('fashioned', '*'), ('*', '*'), ('*', '*'), ('*', '*'), ('*', '*'), ('*', '*'), ('*', ','), (',', 'then'), ('then', 'try'), ('try', 'our'), ('our', 'brand'), ('brand', 'new'), ('new', 'site'), ('site', ';'), (';', 'it'), ('it', 'was'), ('was', 'developed'), ('developed', 'and'), ('and', 'created'), ('created', 'to'), ('to', 'help'), ('help', 'anyone'), ('anyone', 'find'), ('find', 'what'), ('what', 'they'), ('they', \"'\"), (\"'\", 're'), ('re', 'looking'), ('looking', 'for'), ('for', '.'), ('.', 'a'), ('a', 'quick'), ('quick', 'bio'), ('bio', 'form'), ('form', 'and'), ('and', 'you'), ('you', \"'\"), (\"'\", 're'), ('re', 'on'), ('on', 'the'), ('the', 'road'), ('road', 'to'), ('to', 'satisfaction'), ('satisfaction', 'in'), ('in', 'every'), ('every', 'sense'), ('sense', 'of'), ('of', 'the'), ('the', 'word'), ('word', '.'), ('.', '.'), ('.', '.'), ('.', '.'), ('.', 'no'), ('no', 'matter'), ('matter', 'what'), ('what', 'that'), ('that', 'may'), ('may', 'be'), ('be', '!'), ('!', 'try'), ('try', 'it'), ('it', 'out'), ('out', 'and'), ('and', 'youll'), ('youll', 'be'), ('be', 'amazed'), ('amazed', '.'), ('.', 'have'), ('have', 'a'), ('a', 'terrific'), ('terrific', 'time'), ('time', 'this'), ('this', 'evening'), ('evening', 'copy'), ('copy', 'and'), ('and', 'pa'), ('pa', 'ste'), ('ste', 'the'), ('the', 'add'), ('add', '.'), ('.', 'ress'), ('ress', 'you'), ('you', 'see'), ('see', 'on'), ('on', 'the'), ('the', 'line'), ('line', 'below'), ('below', 'into'), ('into', 'your'), ('your', 'browser'), ('browser', 'to'), ('to', 'come'), ('come', 'to'), ('to', 'the'), ('the', 'site'), ('site', '.'), ('.', 'http'), ('http', ':'), (':', '/'), ('/', '/'), ('/', 'www'), ('www', '.'), ('.', 'meganbang'), ('meganbang', '.'), ('.', 'biz'), ('biz', '/'), ('/', 'bld'), ('bld', '/'), ('/', 'acc'), ('acc', '/'), ('/', 'no'), ('no', 'more'), ('more', 'plz'), ('plz', 'http'), ('http', ':'), (':', '/'), ('/', '/'), ('/', 'www'), ('www', '.'), ('.', 'naturalgolden'), ('naturalgolden', '.'), ('.', 'com'), ('com', '/'), ('/', 'retract'), ('retract', '/'), ('/', 'counterattack'), ('counterattack', 'aitken'), ('aitken', 'step'), ('step', 'preemptive'), ('preemptive', 'shoehorn'), ('shoehorn', 'scaup'), ('scaup', '.'), ('.', 'electrocardiograph'), ('electrocardiograph', 'movie'), ('movie', 'honeycomb'), ('honeycomb', '.'), ('.', 'monster'), ('monster', 'war'), ('war', 'brandywine'), ('brandywine', 'pietism'), ('pietism', 'byrne'), ('byrne', 'catatonia'), ('catatonia', '.'), ('.', 'encomia'), ('encomia', 'lookup'), ('lookup', 'intervenor'), ('intervenor', 'skeleton'), ('skeleton', 'turn'), ('turn', 'catfish'), ('catfish', '.')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RWMvVkyNu4wv",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "outputId": "7f9c5593-1910-4794-96a5-b7107f88037a"
   },
   "source": [
    "#TODO : create trigrams of your email & print it\n",
    "trigrams = "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9cfI8a-u4wv"
   },
   "source": [
    "### Lemmatize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HApbCoxfu4wv"
   },
   "source": [
    "We want to lemmatize the verbatims"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "JJvaPPFXu4wv"
   },
   "source": [
    "#TODO get lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/sebila/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "EJagBrGxu4ww",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "outputId": "4ed3ef4e-6fcf-4c26-9987-8c0abd6ff48c"
   },
   "source": [
    "#TODO print lemmatisation of \"found\" without POS\n",
    "result = lemmatizer.lemmatize('found')\n",
    "print(result)\n",
    "#TODO print lemmatisation of \"found\" with POS\n",
    "result2 = lemmatizer.lemmatize('found', 'v')\n",
    "print(result2)\n",
    "#"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found\n",
      "find\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssEylFmcu4ww"
   },
   "source": [
    "One little subtelty : Lemmatizing efficiently requires to pos_tag the words to know their grammatical nature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKU4eAz2u4ww"
   },
   "source": [
    "### Let's pos_tag the tokens"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "8jX64NQku4ww"
   },
   "source": [
    "#TODO pos_tag your tokenized email & print it\n",
    "pos_tagged_mail = "
   ],
   "execution_count": 6,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (50727860.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn [6], line 2\u001B[0;36m\u001B[0m\n\u001B[0;31m    pos_tagged_mail =\u001B[0m\n\u001B[0m                      ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "KgwKy20zu4ww"
   },
   "source": [
    "# Try to understand\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \"\"\"\n",
    "    Modifies pos_tag to get a more general nature of word\n",
    "    \"\"\"\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return 'v'\n",
    "        #return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lvpdu1QUu4ww"
   },
   "source": [
    "### Lemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PAiK5Aiju4wx",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "outputId": "02ecc63b-7b8b-47c9-ee2c-4119bc67ef40"
   },
   "source": [
    "#TODO: Lemmatize your email without pos & print it\n",
    "lemmatized_mail_no_pos = \n",
    "print(lemmatized_mail_no_pos)"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3181786431.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn [8], line 2\u001B[0;36m\u001B[0m\n\u001B[0;31m    lemmatized_mail_no_pos =\u001B[0m\n\u001B[0m                             ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "kMx_oeknu4wx"
   },
   "source": [
    "##TODO: Lemmatize your email with pos & print it\n",
    "lemmatized_mail = \n",
    "print (lemmatized_mail)\n",
    "                   "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "nAkJYAQou4wx"
   },
   "source": [
    "#TODO: Did we deleted word between tokenization & lemmatization ?\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LApH-UH0u4wx"
   },
   "source": [
    "## Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xrufJtUIu4wx"
   },
   "source": [
    "stemmer = \n",
    "#TODO: stem your email\n",
    "stemmed_email = "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uieldqR_u4wy"
   },
   "source": [
    "#TODO: print the length of your email\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1uO9Ckou4wy"
   },
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "PA4UqCAMu4wy"
   },
   "source": [
    "# Have a look at stopwords\n",
    "stoplist = \n",
    "\n",
    "#TODO: print example of stop words\n",
    "print(stoplist)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "gGCfXBhPu4wy"
   },
   "source": [
    "#TODO: removing stopwords\n",
    "mail_no_stopwords = "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j51uw-Qvu4wy"
   },
   "source": [
    "print(tokenized_mail)\n",
    "print('---------------------------------')\n",
    "print(mail_no_stopwords)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "4o1bNeE3u4wy"
   },
   "source": [
    "#TODO: add some relevant stopwords\n",
    "print(tokenized_mail)\n",
    "print('---------------------------------')\n",
    "print(mail_no_stopwprds)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "EPAOY_EMu4wy"
   },
   "source": [
    "#TODO: removing new stopwords\n",
    "mail_no_stopwords = "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vu2rPSQtu4wz"
   },
   "source": [
    "print(tokenized_mail)\n",
    "print('---------------------------------')\n",
    "print(mail_no_stopwords)\n",
    "print('---------------------------------')\n",
    "print ('Length single email without stopwords = ' + str(len(mail_no_stopwords)) + ' words' )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zL1L_ITTu4wz"
   },
   "source": [
    "### Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "aSf79bn1u4wz"
   },
   "source": [
    "#TODO: create a punctuation list\n",
    "stop_punctuation = ['.','?',',','!']\n",
    "\n",
    "#TODO: removing punctuation & print it\n",
    "mail_clean = \n",
    "print(mail_clean)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sExljiCWu4wz"
   },
   "source": [
    "## B. Preprocessing all emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Qs_OPuCu4wz"
   },
   "source": [
    "### Functions definition "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "id": "PMmjWfxZu4wz"
   },
   "source": [
    "def preprocess(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes, lowers, and stems\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_features(text):\n",
    "\n",
    "def get_features_no_processing(text):\n",
    "  "
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}