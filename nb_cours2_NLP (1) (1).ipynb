{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHnEwgLcu4wj"
      },
      "source": [
        "# <center> TP - Text-Mining <center/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEyABl2ou4wp"
      },
      "source": [
        "** Before you begin this notebook, please make sure that you have the data folder + that you are running this notebook from your workspace **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thnWx-pju4wp"
      },
      "source": [
        "_Now let's practice and use some text-mining techniques!_\n",
        "<br/>\n",
        "<br/> In this notebook we will study a mail dataset.\n",
        "Our final goal is to have some insights about the different subjects that appear in mailboxes. Among other problems, one big issue is that these mailboxes are poluted with a lot of spams.\n",
        "\n",
        "> The sequence we propose :\n",
        "- First you'll work on pre-processing the content of the mails\n",
        "- Then you will try to detect whether a mail is a spam or not through a supervized learning algorithm\n",
        "- Finally, once you've trained an algorithm to detect spams, you will try to identify the main topics in the remaining non-spam mails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUgRDEE-u4wp"
      },
      "source": [
        "## The data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sr9Tx-Cu4wq"
      },
      "source": [
        "Data are separated in six parts, each containing around 5000 mails.\n",
        "<br/> Each of these data chunks are separated in two parts : \n",
        ">- hams : that is to say non-spam mails\n",
        "- spams : containing advertisement or unrelevant content\n",
        "\n",
        "\n",
        "get the zip file here :\n",
        "- https://drive.google.com/file/d/1j1xO3HSevP__ZsP2FvTx7p0UHJO__7rj/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGm7OI0Xu4wq"
      },
      "source": [
        "***\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KCGtBSmu4wq"
      },
      "source": [
        "## 0. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl2w5Sytu4wq",
        "outputId": "c1679fed-895a-4531-8f10-dbe8ed12cf10"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/wailbenfatma/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/wailbenfatma/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/wailbenfatma/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/wailbenfatma/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#Import usefull package\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "#Import nltk packages to manipulate text\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.metrics import ConfusionMatrix\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "from nltk import word_tokenize, WordNetLemmatizer, PorterStemmer\n",
        "from nltk import NaiveBayesClassifier, classify\n",
        "from nltk import pos_tag\n",
        "from nltk import ngrams\n",
        "\n",
        "#***\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk import word_tokenize,sent_tokenize\n",
        "\n",
        "# Let's add a path containing some useful nltk data\n",
        "nltk.data.path += ['/mnt/share/nltk_data']\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "IGS0r7ADu4wr"
      },
      "outputs": [],
      "source": [
        "# Modify the variable hereunder to you repository\n",
        "path_data  = '/Users/wailbenfatma/Documents/work/ESME/NLP/exoCours2/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAwi-13Yu4wr"
      },
      "source": [
        "# I. Data ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "tkcrs9X3u4ws"
      },
      "outputs": [],
      "source": [
        "# Nothing to understand\n",
        "def read_mails(folder):\n",
        "    \"\"\"\n",
        "    Reads all the mails contained in a folder and gathers them in a list\n",
        "    \n",
        "    Args :\n",
        "        folder (str) : path to the folder containing mails\n",
        "        \n",
        "    Returns :\n",
        "        mails_list (list) : a list containing mails contents\n",
        "    \"\"\"\n",
        "    \n",
        "    mails_list = []\n",
        "    files_list = os.listdir(folder)\n",
        "    for file_name in files_list:\n",
        "        file_content = open(folder + file_name, 'r', encoding='latin1')\n",
        "        mails_list.append(file_content.read())\n",
        "    file_content.close()\n",
        "    return mails_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRmfcmhau4ws"
      },
      "source": [
        "## A. Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOXef6eBu4ws"
      },
      "source": [
        "Data are separated in six parts, each containing around 5000 mails.\n",
        "<br/> Each of these data chunks are separated in two parts : ham and spam\n",
        "<br/>Let's load the first data chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "gKLtlTqyu4ws"
      },
      "outputs": [],
      "source": [
        "spams = []\n",
        "hams = []\n",
        "for folder in os.listdir(path_data):\n",
        "    if not folder.startswith('.'):\n",
        "        # Load corre spams\n",
        "        spams.extend(read_mails(path_data + folder + \"/spam/\"))\n",
        "        # Load corresponding hams\n",
        "        hams.extend(read_mails(path_data + folder + \"/ham/\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgAlfQ_Zu4wt"
      },
      "source": [
        "# II. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O01QenXHu4wu"
      },
      "source": [
        "## A. Preprocessing one email"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASh8oqisu4wu"
      },
      "source": [
        "Display one email content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GsrgaUKju4wu",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#Get one spam-email & print it\n",
        "single_email = spams[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg48cXH9xnCa",
        "outputId": "c3bd2bf0-69c3-485c-dd89-63feab86bc09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subject: what up , , your cam babe\n",
            "what are you looking for ?\n",
            "if your looking for a companion for friendship , love , a date , or just good ole '\n",
            "fashioned * * * * * * , then try our brand new site ; it was developed and created\n",
            "to help anyone find what they ' re looking for . a quick bio form and you ' re\n",
            "on the road to satisfaction in every sense of the word . . . . no matter what\n",
            "that may be !\n",
            "try it out and youll be amazed .\n",
            "have a terrific time this evening\n",
            "copy and pa ste the add . ress you see on the line below into your browser to come to the site .\n",
            "http : / / www . meganbang . biz / bld / acc /\n",
            "no more plz\n",
            "http : / / www . naturalgolden . com / retract /\n",
            "counterattack aitken step preemptive shoehorn scaup . electrocardiograph movie honeycomb . monster war brandywine pietism byrne catatonia . encomia lookup intervenor skeleton turn catfish .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(single_email)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rex_JwA1u4wu"
      },
      "source": [
        "### Lower verbatim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDlT0RNlu4wu"
      },
      "source": [
        "Lower the content of the previously displayed mail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Qye7vA8u4wu",
        "outputId": "1117d34e-50a8-418f-877c-7d6876268ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "subject: what up , , your cam babe\n",
            "what are you looking for ?\n",
            "if your looking for a companion for friendship , love , a date , or just good ole '\n",
            "fashioned * * * * * * , then try our brand new site ; it was developed and created\n",
            "to help anyone find what they ' re looking for . a quick bio form and you ' re\n",
            "on the road to satisfaction in every sense of the word . . . . no matter what\n",
            "that may be !\n",
            "try it out and youll be amazed .\n",
            "have a terrific time this evening\n",
            "copy and pa ste the add . ress you see on the line below into your browser to come to the site .\n",
            "http : / / www . meganbang . biz / bld / acc /\n",
            "no more plz\n",
            "http : / / www . naturalgolden . com / retract /\n",
            "counterattack aitken step preemptive shoehorn scaup . electrocardiograph movie honeycomb . monster war brandywine pietism byrne catatonia . encomia lookup intervenor skeleton turn catfish .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#TODO : lower case the email & print it\n",
        "lower_mail = single_email.lower()\n",
        "print(lower_mail)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf_2BNWzu4wu"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6mbksrDu4wv"
      },
      "source": [
        "Here we are using a word tokenizer to divide the sentence into tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YipGifXCu4wv",
        "outputId": "0edca7a3-f41f-4baa-aa48-8c5f62d24edb",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['subject', ':', 'what', 'up', ',', ',', 'your', 'cam', 'babe', 'what', 'are', 'you', 'looking', 'for', '?', 'if', 'your', 'looking', 'for', 'a', 'companion', 'for', 'friendship', ',', 'love', ',', 'a', 'date', ',', 'or', 'just', 'good', 'ole', \"'\", 'fashioned', '*', '*', '*', '*', '*', '*', ',', 'then', 'try', 'our', 'brand', 'new', 'site', ';', 'it', 'was', 'developed', 'and', 'created', 'to', 'help', 'anyone', 'find', 'what', 'they', \"'\", 're', 'looking', 'for', '.', 'a', 'quick', 'bio', 'form', 'and', 'you', \"'\", 're', 'on', 'the', 'road', 'to', 'satisfaction', 'in', 'every', 'sense', 'of', 'the', 'word', '.', '.', '.', '.', 'no', 'matter', 'what', 'that', 'may', 'be', '!', 'try', 'it', 'out', 'and', 'youll', 'be', 'amazed', '.', 'have', 'a', 'terrific', 'time', 'this', 'evening', 'copy', 'and', 'pa', 'ste', 'the', 'add', '.', 'ress', 'you', 'see', 'on', 'the', 'line', 'below', 'into', 'your', 'browser', 'to', 'come', 'to', 'the', 'site', '.', 'http', ':', '/', '/', 'www', '.', 'meganbang', '.', 'biz', '/', 'bld', '/', 'acc', '/', 'no', 'more', 'plz', 'http', ':', '/', '/', 'www', '.', 'naturalgolden', '.', 'com', '/', 'retract', '/', 'counterattack', 'aitken', 'step', 'preemptive', 'shoehorn', 'scaup', '.', 'electrocardiograph', 'movie', 'honeycomb', '.', 'monster', 'war', 'brandywine', 'pietism', 'byrne', 'catatonia', '.', 'encomia', 'lookup', 'intervenor', 'skeleton', 'turn', 'catfish', '.']\n"
          ]
        }
      ],
      "source": [
        "#TODO : tokenize your email and print it\n",
        "tokenized_mail = word_tokenize(lower_mail)\n",
        "print(tokenized_mail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "eYWnd6xJu4wv",
        "outputId": "17c46145-baa5-4230-e297-04d299b28bb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('subject', ':'),\n",
              " (':', 'what'),\n",
              " ('what', 'up'),\n",
              " ('up', ','),\n",
              " (',', ','),\n",
              " (',', 'your'),\n",
              " ('your', 'cam'),\n",
              " ('cam', 'babe'),\n",
              " ('babe', 'what'),\n",
              " ('what', 'are'),\n",
              " ('are', 'you'),\n",
              " ('you', 'looking'),\n",
              " ('looking', 'for'),\n",
              " ('for', '?'),\n",
              " ('?', 'if'),\n",
              " ('if', 'your'),\n",
              " ('your', 'looking'),\n",
              " ('looking', 'for'),\n",
              " ('for', 'a'),\n",
              " ('a', 'companion'),\n",
              " ('companion', 'for'),\n",
              " ('for', 'friendship'),\n",
              " ('friendship', ','),\n",
              " (',', 'love'),\n",
              " ('love', ','),\n",
              " (',', 'a'),\n",
              " ('a', 'date'),\n",
              " ('date', ','),\n",
              " (',', 'or'),\n",
              " ('or', 'just'),\n",
              " ('just', 'good'),\n",
              " ('good', 'ole'),\n",
              " ('ole', \"'\"),\n",
              " (\"'\", 'fashioned'),\n",
              " ('fashioned', '*'),\n",
              " ('*', '*'),\n",
              " ('*', '*'),\n",
              " ('*', '*'),\n",
              " ('*', '*'),\n",
              " ('*', '*'),\n",
              " ('*', ','),\n",
              " (',', 'then'),\n",
              " ('then', 'try'),\n",
              " ('try', 'our'),\n",
              " ('our', 'brand'),\n",
              " ('brand', 'new'),\n",
              " ('new', 'site'),\n",
              " ('site', ';'),\n",
              " (';', 'it'),\n",
              " ('it', 'was'),\n",
              " ('was', 'developed'),\n",
              " ('developed', 'and'),\n",
              " ('and', 'created'),\n",
              " ('created', 'to'),\n",
              " ('to', 'help'),\n",
              " ('help', 'anyone'),\n",
              " ('anyone', 'find'),\n",
              " ('find', 'what'),\n",
              " ('what', 'they'),\n",
              " ('they', \"'\"),\n",
              " (\"'\", 're'),\n",
              " ('re', 'looking'),\n",
              " ('looking', 'for'),\n",
              " ('for', '.'),\n",
              " ('.', 'a'),\n",
              " ('a', 'quick'),\n",
              " ('quick', 'bio'),\n",
              " ('bio', 'form'),\n",
              " ('form', 'and'),\n",
              " ('and', 'you'),\n",
              " ('you', \"'\"),\n",
              " (\"'\", 're'),\n",
              " ('re', 'on'),\n",
              " ('on', 'the'),\n",
              " ('the', 'road'),\n",
              " ('road', 'to'),\n",
              " ('to', 'satisfaction'),\n",
              " ('satisfaction', 'in'),\n",
              " ('in', 'every'),\n",
              " ('every', 'sense'),\n",
              " ('sense', 'of'),\n",
              " ('of', 'the'),\n",
              " ('the', 'word'),\n",
              " ('word', '.'),\n",
              " ('.', '.'),\n",
              " ('.', '.'),\n",
              " ('.', '.'),\n",
              " ('.', 'no'),\n",
              " ('no', 'matter'),\n",
              " ('matter', 'what'),\n",
              " ('what', 'that'),\n",
              " ('that', 'may'),\n",
              " ('may', 'be'),\n",
              " ('be', '!'),\n",
              " ('!', 'try'),\n",
              " ('try', 'it'),\n",
              " ('it', 'out'),\n",
              " ('out', 'and'),\n",
              " ('and', 'youll'),\n",
              " ('youll', 'be'),\n",
              " ('be', 'amazed'),\n",
              " ('amazed', '.'),\n",
              " ('.', 'have'),\n",
              " ('have', 'a'),\n",
              " ('a', 'terrific'),\n",
              " ('terrific', 'time'),\n",
              " ('time', 'this'),\n",
              " ('this', 'evening'),\n",
              " ('evening', 'copy'),\n",
              " ('copy', 'and'),\n",
              " ('and', 'pa'),\n",
              " ('pa', 'ste'),\n",
              " ('ste', 'the'),\n",
              " ('the', 'add'),\n",
              " ('add', '.'),\n",
              " ('.', 'ress'),\n",
              " ('ress', 'you'),\n",
              " ('you', 'see'),\n",
              " ('see', 'on'),\n",
              " ('on', 'the'),\n",
              " ('the', 'line'),\n",
              " ('line', 'below'),\n",
              " ('below', 'into'),\n",
              " ('into', 'your'),\n",
              " ('your', 'browser'),\n",
              " ('browser', 'to'),\n",
              " ('to', 'come'),\n",
              " ('come', 'to'),\n",
              " ('to', 'the'),\n",
              " ('the', 'site'),\n",
              " ('site', '.'),\n",
              " ('.', 'http'),\n",
              " ('http', ':'),\n",
              " (':', '/'),\n",
              " ('/', '/'),\n",
              " ('/', 'www'),\n",
              " ('www', '.'),\n",
              " ('.', 'meganbang'),\n",
              " ('meganbang', '.'),\n",
              " ('.', 'biz'),\n",
              " ('biz', '/'),\n",
              " ('/', 'bld'),\n",
              " ('bld', '/'),\n",
              " ('/', 'acc'),\n",
              " ('acc', '/'),\n",
              " ('/', 'no'),\n",
              " ('no', 'more'),\n",
              " ('more', 'plz'),\n",
              " ('plz', 'http'),\n",
              " ('http', ':'),\n",
              " (':', '/'),\n",
              " ('/', '/'),\n",
              " ('/', 'www'),\n",
              " ('www', '.'),\n",
              " ('.', 'naturalgolden'),\n",
              " ('naturalgolden', '.'),\n",
              " ('.', 'com'),\n",
              " ('com', '/'),\n",
              " ('/', 'retract'),\n",
              " ('retract', '/'),\n",
              " ('/', 'counterattack'),\n",
              " ('counterattack', 'aitken'),\n",
              " ('aitken', 'step'),\n",
              " ('step', 'preemptive'),\n",
              " ('preemptive', 'shoehorn'),\n",
              " ('shoehorn', 'scaup'),\n",
              " ('scaup', '.'),\n",
              " ('.', 'electrocardiograph'),\n",
              " ('electrocardiograph', 'movie'),\n",
              " ('movie', 'honeycomb'),\n",
              " ('honeycomb', '.'),\n",
              " ('.', 'monster'),\n",
              " ('monster', 'war'),\n",
              " ('war', 'brandywine'),\n",
              " ('brandywine', 'pietism'),\n",
              " ('pietism', 'byrne'),\n",
              " ('byrne', 'catatonia'),\n",
              " ('catatonia', '.'),\n",
              " ('.', 'encomia'),\n",
              " ('encomia', 'lookup'),\n",
              " ('lookup', 'intervenor'),\n",
              " ('intervenor', 'skeleton'),\n",
              " ('skeleton', 'turn'),\n",
              " ('turn', 'catfish'),\n",
              " ('catfish', '.')]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#TODO : create bigrams of your email & print it\n",
        "bigrams = [bigram for bigram in ngrams(tokenized_mail,2)]\n",
        "bigrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "RWMvVkyNu4wv",
        "outputId": "7f9c5593-1910-4794-96a5-b7107f88037a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('subject', ':', 'what'),\n",
              " (':', 'what', 'up'),\n",
              " ('what', 'up', ','),\n",
              " ('up', ',', ','),\n",
              " (',', ',', 'your'),\n",
              " (',', 'your', 'cam'),\n",
              " ('your', 'cam', 'babe'),\n",
              " ('cam', 'babe', 'what'),\n",
              " ('babe', 'what', 'are'),\n",
              " ('what', 'are', 'you'),\n",
              " ('are', 'you', 'looking'),\n",
              " ('you', 'looking', 'for'),\n",
              " ('looking', 'for', '?'),\n",
              " ('for', '?', 'if'),\n",
              " ('?', 'if', 'your'),\n",
              " ('if', 'your', 'looking'),\n",
              " ('your', 'looking', 'for'),\n",
              " ('looking', 'for', 'a'),\n",
              " ('for', 'a', 'companion'),\n",
              " ('a', 'companion', 'for'),\n",
              " ('companion', 'for', 'friendship'),\n",
              " ('for', 'friendship', ','),\n",
              " ('friendship', ',', 'love'),\n",
              " (',', 'love', ','),\n",
              " ('love', ',', 'a'),\n",
              " (',', 'a', 'date'),\n",
              " ('a', 'date', ','),\n",
              " ('date', ',', 'or'),\n",
              " (',', 'or', 'just'),\n",
              " ('or', 'just', 'good'),\n",
              " ('just', 'good', 'ole'),\n",
              " ('good', 'ole', \"'\"),\n",
              " ('ole', \"'\", 'fashioned'),\n",
              " (\"'\", 'fashioned', '*'),\n",
              " ('fashioned', '*', '*'),\n",
              " ('*', '*', '*'),\n",
              " ('*', '*', '*'),\n",
              " ('*', '*', '*'),\n",
              " ('*', '*', '*'),\n",
              " ('*', '*', ','),\n",
              " ('*', ',', 'then'),\n",
              " (',', 'then', 'try'),\n",
              " ('then', 'try', 'our'),\n",
              " ('try', 'our', 'brand'),\n",
              " ('our', 'brand', 'new'),\n",
              " ('brand', 'new', 'site'),\n",
              " ('new', 'site', ';'),\n",
              " ('site', ';', 'it'),\n",
              " (';', 'it', 'was'),\n",
              " ('it', 'was', 'developed'),\n",
              " ('was', 'developed', 'and'),\n",
              " ('developed', 'and', 'created'),\n",
              " ('and', 'created', 'to'),\n",
              " ('created', 'to', 'help'),\n",
              " ('to', 'help', 'anyone'),\n",
              " ('help', 'anyone', 'find'),\n",
              " ('anyone', 'find', 'what'),\n",
              " ('find', 'what', 'they'),\n",
              " ('what', 'they', \"'\"),\n",
              " ('they', \"'\", 're'),\n",
              " (\"'\", 're', 'looking'),\n",
              " ('re', 'looking', 'for'),\n",
              " ('looking', 'for', '.'),\n",
              " ('for', '.', 'a'),\n",
              " ('.', 'a', 'quick'),\n",
              " ('a', 'quick', 'bio'),\n",
              " ('quick', 'bio', 'form'),\n",
              " ('bio', 'form', 'and'),\n",
              " ('form', 'and', 'you'),\n",
              " ('and', 'you', \"'\"),\n",
              " ('you', \"'\", 're'),\n",
              " (\"'\", 're', 'on'),\n",
              " ('re', 'on', 'the'),\n",
              " ('on', 'the', 'road'),\n",
              " ('the', 'road', 'to'),\n",
              " ('road', 'to', 'satisfaction'),\n",
              " ('to', 'satisfaction', 'in'),\n",
              " ('satisfaction', 'in', 'every'),\n",
              " ('in', 'every', 'sense'),\n",
              " ('every', 'sense', 'of'),\n",
              " ('sense', 'of', 'the'),\n",
              " ('of', 'the', 'word'),\n",
              " ('the', 'word', '.'),\n",
              " ('word', '.', '.'),\n",
              " ('.', '.', '.'),\n",
              " ('.', '.', '.'),\n",
              " ('.', '.', 'no'),\n",
              " ('.', 'no', 'matter'),\n",
              " ('no', 'matter', 'what'),\n",
              " ('matter', 'what', 'that'),\n",
              " ('what', 'that', 'may'),\n",
              " ('that', 'may', 'be'),\n",
              " ('may', 'be', '!'),\n",
              " ('be', '!', 'try'),\n",
              " ('!', 'try', 'it'),\n",
              " ('try', 'it', 'out'),\n",
              " ('it', 'out', 'and'),\n",
              " ('out', 'and', 'youll'),\n",
              " ('and', 'youll', 'be'),\n",
              " ('youll', 'be', 'amazed'),\n",
              " ('be', 'amazed', '.'),\n",
              " ('amazed', '.', 'have'),\n",
              " ('.', 'have', 'a'),\n",
              " ('have', 'a', 'terrific'),\n",
              " ('a', 'terrific', 'time'),\n",
              " ('terrific', 'time', 'this'),\n",
              " ('time', 'this', 'evening'),\n",
              " ('this', 'evening', 'copy'),\n",
              " ('evening', 'copy', 'and'),\n",
              " ('copy', 'and', 'pa'),\n",
              " ('and', 'pa', 'ste'),\n",
              " ('pa', 'ste', 'the'),\n",
              " ('ste', 'the', 'add'),\n",
              " ('the', 'add', '.'),\n",
              " ('add', '.', 'ress'),\n",
              " ('.', 'ress', 'you'),\n",
              " ('ress', 'you', 'see'),\n",
              " ('you', 'see', 'on'),\n",
              " ('see', 'on', 'the'),\n",
              " ('on', 'the', 'line'),\n",
              " ('the', 'line', 'below'),\n",
              " ('line', 'below', 'into'),\n",
              " ('below', 'into', 'your'),\n",
              " ('into', 'your', 'browser'),\n",
              " ('your', 'browser', 'to'),\n",
              " ('browser', 'to', 'come'),\n",
              " ('to', 'come', 'to'),\n",
              " ('come', 'to', 'the'),\n",
              " ('to', 'the', 'site'),\n",
              " ('the', 'site', '.'),\n",
              " ('site', '.', 'http'),\n",
              " ('.', 'http', ':'),\n",
              " ('http', ':', '/'),\n",
              " (':', '/', '/'),\n",
              " ('/', '/', 'www'),\n",
              " ('/', 'www', '.'),\n",
              " ('www', '.', 'meganbang'),\n",
              " ('.', 'meganbang', '.'),\n",
              " ('meganbang', '.', 'biz'),\n",
              " ('.', 'biz', '/'),\n",
              " ('biz', '/', 'bld'),\n",
              " ('/', 'bld', '/'),\n",
              " ('bld', '/', 'acc'),\n",
              " ('/', 'acc', '/'),\n",
              " ('acc', '/', 'no'),\n",
              " ('/', 'no', 'more'),\n",
              " ('no', 'more', 'plz'),\n",
              " ('more', 'plz', 'http'),\n",
              " ('plz', 'http', ':'),\n",
              " ('http', ':', '/'),\n",
              " (':', '/', '/'),\n",
              " ('/', '/', 'www'),\n",
              " ('/', 'www', '.'),\n",
              " ('www', '.', 'naturalgolden'),\n",
              " ('.', 'naturalgolden', '.'),\n",
              " ('naturalgolden', '.', 'com'),\n",
              " ('.', 'com', '/'),\n",
              " ('com', '/', 'retract'),\n",
              " ('/', 'retract', '/'),\n",
              " ('retract', '/', 'counterattack'),\n",
              " ('/', 'counterattack', 'aitken'),\n",
              " ('counterattack', 'aitken', 'step'),\n",
              " ('aitken', 'step', 'preemptive'),\n",
              " ('step', 'preemptive', 'shoehorn'),\n",
              " ('preemptive', 'shoehorn', 'scaup'),\n",
              " ('shoehorn', 'scaup', '.'),\n",
              " ('scaup', '.', 'electrocardiograph'),\n",
              " ('.', 'electrocardiograph', 'movie'),\n",
              " ('electrocardiograph', 'movie', 'honeycomb'),\n",
              " ('movie', 'honeycomb', '.'),\n",
              " ('honeycomb', '.', 'monster'),\n",
              " ('.', 'monster', 'war'),\n",
              " ('monster', 'war', 'brandywine'),\n",
              " ('war', 'brandywine', 'pietism'),\n",
              " ('brandywine', 'pietism', 'byrne'),\n",
              " ('pietism', 'byrne', 'catatonia'),\n",
              " ('byrne', 'catatonia', '.'),\n",
              " ('catatonia', '.', 'encomia'),\n",
              " ('.', 'encomia', 'lookup'),\n",
              " ('encomia', 'lookup', 'intervenor'),\n",
              " ('lookup', 'intervenor', 'skeleton'),\n",
              " ('intervenor', 'skeleton', 'turn'),\n",
              " ('skeleton', 'turn', 'catfish'),\n",
              " ('turn', 'catfish', '.')]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#TODO : create trigrams of your email & print it\n",
        "trigrams = [trigram for trigram in ngrams(tokenized_mail,3)]\n",
        "trigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9cfI8a-u4wv"
      },
      "source": [
        "### Lemmatize "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HApbCoxfu4wv"
      },
      "source": [
        "We want to lemmatize the verbatims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "id": "JJvaPPFXu4wv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /Users/wailbenfatma/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('omw-1.4')\n",
        "\n",
        "#TODO get lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "EJagBrGxu4ww",
        "outputId": "4ed3ef4e-6fcf-4c26-9987-8c0abd6ff48c",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "found\n",
            "find\n"
          ]
        }
      ],
      "source": [
        "#TODO print lemmatisation of \"found\" without POS\n",
        "result = lemmatizer.lemmatize(\"found\")\n",
        "print(result)\n",
        "#TODO print lemmatisation of \"found\" with POS\n",
        "result2 = lemmatizer.lemmatize(\"found\",'v')\n",
        "print(result2)\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssEylFmcu4ww"
      },
      "source": [
        "One little subtelty : Lemmatizing efficiently requires to pos_tag the words to know their grammatical nature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKU4eAz2u4ww"
      },
      "source": [
        "### Let's pos_tag the tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8jX64NQku4ww",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('subject', 'NN'), (':', ':'), ('what', 'WP'), ('up', 'IN'), (',', ','), (',', ','), ('your', 'PRP$'), ('cam', 'NN'), ('babe', 'IN'), ('what', 'WP'), ('are', 'VBP'), ('you', 'PRP'), ('looking', 'VBG'), ('for', 'IN'), ('?', '.'), ('if', 'IN'), ('your', 'PRP$'), ('looking', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('companion', 'NN'), ('for', 'IN'), ('friendship', 'NN'), (',', ','), ('love', 'NN'), (',', ','), ('a', 'DT'), ('date', 'NN'), (',', ','), ('or', 'CC'), ('just', 'RB'), ('good', 'JJ'), ('ole', 'NN'), (\"'\", 'POS'), ('fashioned', 'VBN'), ('*', 'NNP'), ('*', 'NNP'), ('*', 'NNP'), ('*', 'NNP'), ('*', 'NNP'), ('*', 'NNP'), (',', ','), ('then', 'RB'), ('try', 'VB'), ('our', 'PRP$'), ('brand', 'NN'), ('new', 'JJ'), ('site', 'NN'), (';', ':'), ('it', 'PRP'), ('was', 'VBD'), ('developed', 'VBN'), ('and', 'CC'), ('created', 'VBN'), ('to', 'TO'), ('help', 'VB'), ('anyone', 'NN'), ('find', 'VB'), ('what', 'WP'), ('they', 'PRP'), (\"'\", 'VBP'), ('re', 'JJ'), ('looking', 'VBG'), ('for', 'IN'), ('.', '.'), ('a', 'DT'), ('quick', 'JJ'), ('bio', 'NN'), ('form', 'NN'), ('and', 'CC'), ('you', 'PRP'), (\"'\", \"''\"), ('re', 'VB'), ('on', 'IN'), ('the', 'DT'), ('road', 'NN'), ('to', 'TO'), ('satisfaction', 'NN'), ('in', 'IN'), ('every', 'DT'), ('sense', 'NN'), ('of', 'IN'), ('the', 'DT'), ('word', 'NN'), ('.', '.'), ('.', '.'), ('.', '.'), ('.', '.'), ('no', 'DT'), ('matter', 'NN'), ('what', 'WP'), ('that', 'WDT'), ('may', 'MD'), ('be', 'VB'), ('!', '.'), ('try', 'VB'), ('it', 'PRP'), ('out', 'RP'), ('and', 'CC'), ('youll', 'RB'), ('be', 'VB'), ('amazed', 'VBN'), ('.', '.'), ('have', 'VB'), ('a', 'DT'), ('terrific', 'JJ'), ('time', 'NN'), ('this', 'DT'), ('evening', 'NN'), ('copy', 'NN'), ('and', 'CC'), ('pa', 'NN'), ('ste', 'NN'), ('the', 'DT'), ('add', 'NN'), ('.', '.'), ('ress', 'NN'), ('you', 'PRP'), ('see', 'VBP'), ('on', 'IN'), ('the', 'DT'), ('line', 'NN'), ('below', 'IN'), ('into', 'IN'), ('your', 'PRP$'), ('browser', 'NN'), ('to', 'TO'), ('come', 'VB'), ('to', 'TO'), ('the', 'DT'), ('site', 'NN'), ('.', '.'), ('http', 'NN'), (':', ':'), ('/', 'JJ'), ('/', 'NNP'), ('www', 'NN'), ('.', '.'), ('meganbang', 'NN'), ('.', '.'), ('biz', 'NN'), ('/', 'JJ'), ('bld', 'NN'), ('/', 'NNP'), ('acc', 'NN'), ('/', 'VBZ'), ('no', 'DT'), ('more', 'RBR'), ('plz', 'JJ'), ('http', 'NN'), (':', ':'), ('/', 'JJ'), ('/', 'NNP'), ('www', 'NN'), ('.', '.'), ('naturalgolden', 'JJ'), ('.', '.'), ('com', 'NN'), ('/', 'JJ'), ('retract', 'NN'), ('/', 'NNP'), ('counterattack', 'NN'), ('aitken', 'JJ'), ('step', 'NN'), ('preemptive', 'JJ'), ('shoehorn', 'JJ'), ('scaup', 'NN'), ('.', '.'), ('electrocardiograph', 'JJ'), ('movie', 'NN'), ('honeycomb', 'NN'), ('.', '.'), ('monster', 'NN'), ('war', 'NN'), ('brandywine', 'NN'), ('pietism', 'NN'), ('byrne', 'JJ'), ('catatonia', 'NN'), ('.', '.'), ('encomia', 'NN'), ('lookup', 'NN'), ('intervenor', 'IN'), ('skeleton', 'NN'), ('turn', 'VBP'), ('catfish', 'JJ'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "#TODO pos_tag your tokenized email & print it\n",
        "pos_tagged_mail = pos_tag(tokenized_mail)\n",
        "print(pos_tagged_mail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": true,
        "id": "KgwKy20zu4ww"
      },
      "outputs": [],
      "source": [
        "# Try to understand\n",
        "def get_wordnet_pos(pos_tag):\n",
        "    \"\"\"\n",
        "    Modifies pos_tag to get a more general nature of word\n",
        "    \"\"\"\n",
        "    if pos_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif pos_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif pos_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif pos_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return 'n'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvpdu1QUu4ww"
      },
      "source": [
        "### Lemmatizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "PAiK5Aiju4wx",
        "outputId": "02ecc63b-7b8b-47c9-ee2c-4119bc67ef40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['subject', ':', 'what', 'up', ',', ',', 'your', 'cam', 'babe', 'what', 'are', 'you', 'looking', 'for', '?', 'if', 'your', 'looking', 'for', 'a', 'companion', 'for', 'friendship', ',', 'love', ',', 'a', 'date', ',', 'or', 'just', 'good', 'ole', \"'\", 'fashioned', '*', '*', '*', '*', '*', '*', ',', 'then', 'try', 'our', 'brand', 'new', 'site', ';', 'it', 'wa', 'developed', 'and', 'created', 'to', 'help', 'anyone', 'find', 'what', 'they', \"'\", 're', 'looking', 'for', '.', 'a', 'quick', 'bio', 'form', 'and', 'you', \"'\", 're', 'on', 'the', 'road', 'to', 'satisfaction', 'in', 'every', 'sense', 'of', 'the', 'word', '.', '.', '.', '.', 'no', 'matter', 'what', 'that', 'may', 'be', '!', 'try', 'it', 'out', 'and', 'youll', 'be', 'amazed', '.', 'have', 'a', 'terrific', 'time', 'this', 'evening', 'copy', 'and', 'pa', 'ste', 'the', 'add', '.', 'res', 'you', 'see', 'on', 'the', 'line', 'below', 'into', 'your', 'browser', 'to', 'come', 'to', 'the', 'site', '.', 'http', ':', '/', '/', 'www', '.', 'meganbang', '.', 'biz', '/', 'bld', '/', 'acc', '/', 'no', 'more', 'plz', 'http', ':', '/', '/', 'www', '.', 'naturalgolden', '.', 'com', '/', 'retract', '/', 'counterattack', 'aitken', 'step', 'preemptive', 'shoehorn', 'scaup', '.', 'electrocardiograph', 'movie', 'honeycomb', '.', 'monster', 'war', 'brandywine', 'pietism', 'byrne', 'catatonia', '.', 'encomium', 'lookup', 'intervenor', 'skeleton', 'turn', 'catfish', '.']\n"
          ]
        }
      ],
      "source": [
        "#TODO: Lemmatize your email without pos & print it\n",
        "lemmatized_mail_no_pos = [lemmatizer.lemmatize(token) for token in tokenized_mail]\n",
        "print(lemmatized_mail_no_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kMx_oeknu4wx",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['subject', ':', 'what', 'up', ',', ',', 'your', 'cam', 'babe', 'what', 'be', 'you', 'look', 'for', '?', 'if', 'your', 'look', 'for', 'a', 'companion', 'for', 'friendship', ',', 'love', ',', 'a', 'date', ',', 'or', 'just', 'good', 'ole', \"'\", 'fashion', '*', '*', '*', '*', '*', '*', ',', 'then', 'try', 'our', 'brand', 'new', 'site', ';', 'it', 'be', 'develop', 'and', 'create', 'to', 'help', 'anyone', 'find', 'what', 'they', \"'\", 're', 'look', 'for', '.', 'a', 'quick', 'bio', 'form', 'and', 'you', \"'\", 're', 'on', 'the', 'road', 'to', 'satisfaction', 'in', 'every', 'sense', 'of', 'the', 'word', '.', '.', '.', '.', 'no', 'matter', 'what', 'that', 'may', 'be', '!', 'try', 'it', 'out', 'and', 'youll', 'be', 'amaze', '.', 'have', 'a', 'terrific', 'time', 'this', 'evening', 'copy', 'and', 'pa', 'ste', 'the', 'add', '.', 'res', 'you', 'see', 'on', 'the', 'line', 'below', 'into', 'your', 'browser', 'to', 'come', 'to', 'the', 'site', '.', 'http', ':', '/', '/', 'www', '.', 'meganbang', '.', 'biz', '/', 'bld', '/', 'acc', '/', 'no', 'more', 'plz', 'http', ':', '/', '/', 'www', '.', 'naturalgolden', '.', 'com', '/', 'retract', '/', 'counterattack', 'aitken', 'step', 'preemptive', 'shoehorn', 'scaup', '.', 'electrocardiograph', 'movie', 'honeycomb', '.', 'monster', 'war', 'brandywine', 'pietism', 'byrne', 'catatonia', '.', 'encomium', 'lookup', 'intervenor', 'skeleton', 'turn', 'catfish', '.']\n"
          ]
        }
      ],
      "source": [
        "##TODO: Lemmatize your email with pos & print it\n",
        "lemmatized_mail = [lemmatizer.lemmatize(word[0],get_wordnet_pos(word[1])) for word in pos_tagged_mail]\n",
        "print (lemmatized_mail)\n",
        "                   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "nAkJYAQou4wx",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "186\n",
            "186\n"
          ]
        }
      ],
      "source": [
        "#TODO: Did we deleted word between tokenization & lemmatization ?\n",
        "print(len(lemmatized_mail_no_pos))\n",
        "print(len(lemmatized_mail))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LApH-UH0u4wx"
      },
      "source": [
        "## Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "xrufJtUIu4wx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['subject', ':', 'what', 'up', ',', ',', 'your', 'cam', 'babe', 'what', 'are', 'you', 'look', 'for', '?', 'if', 'your', 'look', 'for', 'a', 'companion', 'for', 'friendship', ',', 'love', ',', 'a', 'date', ',', 'or', 'just', 'good', 'ole', \"'\", 'fashion', '*', '*', '*', '*', '*', '*', ',', 'then', 'tri', 'our', 'brand', 'new', 'site', ';', 'it', 'wa', 'develop', 'and', 'creat', 'to', 'help', 'anyon', 'find', 'what', 'they', \"'\", 're', 'look', 'for', '.', 'a', 'quick', 'bio', 'form', 'and', 'you', \"'\", 're', 'on', 'the', 'road', 'to', 'satisfact', 'in', 'everi', 'sens', 'of', 'the', 'word', '.', '.', '.', '.', 'no', 'matter', 'what', 'that', 'may', 'be', '!', 'tri', 'it', 'out', 'and', 'youll', 'be', 'amaz', '.', 'have', 'a', 'terrif', 'time', 'thi', 'even', 'copi', 'and', 'pa', 'ste', 'the', 'add', '.', 'ress', 'you', 'see', 'on', 'the', 'line', 'below', 'into', 'your', 'browser', 'to', 'come', 'to', 'the', 'site', '.', 'http', ':', '/', '/', 'www', '.', 'meganbang', '.', 'biz', '/', 'bld', '/', 'acc', '/', 'no', 'more', 'plz', 'http', ':', '/', '/', 'www', '.', 'naturalgolden', '.', 'com', '/', 'retract', '/', 'counterattack', 'aitken', 'step', 'preemptiv', 'shoehorn', 'scaup', '.', 'electrocardiograph', 'movi', 'honeycomb', '.', 'monster', 'war', 'brandywin', 'pietism', 'byrn', 'catatonia', '.', 'encomia', 'lookup', 'intervenor', 'skeleton', 'turn', 'catfish', '.']\n"
          ]
        }
      ],
      "source": [
        "stemmer = PorterStemmer()\n",
        "#TODO: stem your email\n",
        "stemmed_email = [stemmer.stem(token) for token in tokenized_mail] \n",
        "print(stemmed_email)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "uieldqR_u4wy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "186\n"
          ]
        }
      ],
      "source": [
        "#TODO: print the length of your email\n",
        "print(len(stemmed_email))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1uO9Ckou4wy"
      },
      "source": [
        "### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "PA4UqCAMu4wy",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "# Have a look at stopwords\n",
        "stoplist = stopwords.words('english')\n",
        "\n",
        "#TODO: print example of stop words\n",
        "print(stoplist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gGCfXBhPu4wy",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['subject', ':', ',', ',', 'cam', 'babe', 'looking', '?', 'looking', 'companion', 'friendship', ',', 'love', ',', 'date', ',', 'good', 'ole', \"'\", 'fashioned', '*', '*', '*', '*', '*', '*', ',', 'try', 'brand', 'new', 'site', ';', 'developed', 'created', 'help', 'anyone', 'find', \"'\", 'looking', '.', 'quick', 'bio', 'form', \"'\", 'road', 'satisfaction', 'every', 'sense', 'word', '.', '.', '.', '.', 'matter', 'may', '!', 'try', 'youll', 'amazed', '.', 'terrific', 'time', 'evening', 'copy', 'pa', 'ste', 'add', '.', 'ress', 'see', 'line', 'browser', 'come', 'site', '.', 'http', ':', '/', '/', 'www', '.', 'meganbang', '.', 'biz', '/', 'bld', '/', 'acc', '/', 'plz', 'http', ':', '/', '/', 'www', '.', 'naturalgolden', '.', 'com', '/', 'retract', '/', 'counterattack', 'aitken', 'step', 'preemptive', 'shoehorn', 'scaup', '.', 'electrocardiograph', 'movie', 'honeycomb', '.', 'monster', 'war', 'brandywine', 'pietism', 'byrne', 'catatonia', '.', 'encomia', 'lookup', 'intervenor', 'skeleton', 'turn', 'catfish', '.']\n"
          ]
        }
      ],
      "source": [
        "#TODO: removing stopwords\n",
        "mail_no_stopwords = [word for word in tokenized_mail if word not in stoplist]\n",
        "print(mail_no_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "j51uw-Qvu4wy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['subject', ':', 'what', 'up', ',', ',', 'your', 'cam', 'babe', 'what', 'are', 'you', 'looking', 'for', '?', 'if', 'your', 'looking', 'for', 'a', 'companion', 'for', 'friendship', ',', 'love', ',', 'a', 'date', ',', 'or', 'just', 'good', 'ole', \"'\", 'fashioned', '*', '*', '*', '*', '*', '*', ',', 'then', 'try', 'our', 'brand', 'new', 'site', ';', 'it', 'was', 'developed', 'and', 'created', 'to', 'help', 'anyone', 'find', 'what', 'they', \"'\", 're', 'looking', 'for', '.', 'a', 'quick', 'bio', 'form', 'and', 'you', \"'\", 're', 'on', 'the', 'road', 'to', 'satisfaction', 'in', 'every', 'sense', 'of', 'the', 'word', '.', '.', '.', '.', 'no', 'matter', 'what', 'that', 'may', 'be', '!', 'try', 'it', 'out', 'and', 'youll', 'be', 'amazed', '.', 'have', 'a', 'terrific', 'time', 'this', 'evening', 'copy', 'and', 'pa', 'ste', 'the', 'add', '.', 'ress', 'you', 'see', 'on', 'the', 'line', 'below', 'into', 'your', 'browser', 'to', 'come', 'to', 'the', 'site', '.', 'http', ':', '/', '/', 'www', '.', 'meganbang', '.', 'biz', '/', 'bld', '/', 'acc', '/', 'no', 'more', 'plz', 'http', ':', '/', '/', 'www', '.', 'naturalgolden', '.', 'com', '/', 'retract', '/', 'counterattack', 'aitken', 'step', 'preemptive', 'shoehorn', 'scaup', '.', 'electrocardiograph', 'movie', 'honeycomb', '.', 'monster', 'war', 'brandywine', 'pietism', 'byrne', 'catatonia', '.', 'encomia', 'lookup', 'intervenor', 'skeleton', 'turn', 'catfish', '.']\n",
            "---------------------------------\n",
            "['subject', ':', ',', ',', 'cam', 'babe', 'looking', '?', 'looking', 'companion', 'friendship', ',', 'love', ',', 'date', ',', 'good', 'ole', \"'\", 'fashioned', '*', '*', '*', '*', '*', '*', ',', 'try', 'brand', 'new', 'site', ';', 'developed', 'created', 'help', 'anyone', 'find', \"'\", 'looking', '.', 'quick', 'bio', 'form', \"'\", 'road', 'satisfaction', 'every', 'sense', 'word', '.', '.', '.', '.', 'matter', 'may', '!', 'try', 'youll', 'amazed', '.', 'terrific', 'time', 'evening', 'copy', 'pa', 'ste', 'add', '.', 'ress', 'see', 'line', 'browser', 'come', 'site', '.', 'http', ':', '/', '/', 'www', '.', 'meganbang', '.', 'biz', '/', 'bld', '/', 'acc', '/', 'plz', 'http', ':', '/', '/', 'www', '.', 'naturalgolden', '.', 'com', '/', 'retract', '/', 'counterattack', 'aitken', 'step', 'preemptive', 'shoehorn', 'scaup', '.', 'electrocardiograph', 'movie', 'honeycomb', '.', 'monster', 'war', 'brandywine', 'pietism', 'byrne', 'catatonia', '.', 'encomia', 'lookup', 'intervenor', 'skeleton', 'turn', 'catfish', '.']\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_mail)\n",
        "print('---------------------------------')\n",
        "print(mail_no_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4o1bNeE3u4wy"
      },
      "outputs": [],
      "source": [
        "#TODO: add some relevant stopwords\n",
        "print(tokenized_mail)\n",
        "print('---------------------------------')\n",
        "print(mail_no_stopwprds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EPAOY_EMu4wy"
      },
      "outputs": [],
      "source": [
        "#TODO: removing new stopwords\n",
        "mail_no_stopwords = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vu2rPSQtu4wz"
      },
      "outputs": [],
      "source": [
        "print(tokenized_mail)\n",
        "print('---------------------------------')\n",
        "print(mail_no_stopwords)\n",
        "print('---------------------------------')\n",
        "print ('Length single email without stopwords = ' + str(len(mail_no_stopwords)) + ' words' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zL1L_ITTu4wz"
      },
      "source": [
        "### Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "aSf79bn1u4wz",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['subject', 'what', 'up', 'your', 'cam', 'babe', 'what', 'are', 'you', 'looking', 'for', 'if', 'your', 'looking', 'for', 'a', 'companion', 'for', 'friendship', 'love', 'a', 'date', 'or', 'just', 'good', 'ole', \"'\", 'fashioned', '*', '*', '*', '*', '*', '*', 'then', 'try', 'our', 'brand', 'new', 'site', ';', 'it', 'was', 'developed', 'and', 'created', 'to', 'help', 'anyone', 'find', 'what', 'they', \"'\", 're', 'looking', 'for', 'a', 'quick', 'bio', 'form', 'and', 'you', \"'\", 're', 'on', 'the', 'road', 'to', 'satisfaction', 'in', 'every', 'sense', 'of', 'the', 'word', 'no', 'matter', 'what', 'that', 'may', 'be', 'try', 'it', 'out', 'and', 'youll', 'be', 'amazed', 'have', 'a', 'terrific', 'time', 'this', 'evening', 'copy', 'and', 'pa', 'ste', 'the', 'add', 'ress', 'you', 'see', 'on', 'the', 'line', 'below', 'into', 'your', 'browser', 'to', 'come', 'to', 'the', 'site', 'http', '/', '/', 'www', 'meganbang', 'biz', '/', 'bld', '/', 'acc', '/', 'no', 'more', 'plz', 'http', '/', '/', 'www', 'naturalgolden', 'com', '/', 'retract', '/', 'counterattack', 'aitken', 'step', 'preemptive', 'shoehorn', 'scaup', 'electrocardiograph', 'movie', 'honeycomb', 'monster', 'war', 'brandywine', 'pietism', 'byrne', 'catatonia', 'encomia', 'lookup', 'intervenor', 'skeleton', 'turn', 'catfish']\n"
          ]
        }
      ],
      "source": [
        "#TODO: create a punctuation list\n",
        "stop_punctuation = ['.','?',',','!',':']\n",
        "\n",
        "#TODO: removing punctuation & print it\n",
        "mail_clean = [word for word in tokenized_mail if word not in stop_punctuation]\n",
        "print(mail_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sExljiCWu4wz"
      },
      "source": [
        "## B. Preprocessing all emails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qs_OPuCu4wz"
      },
      "source": [
        "### Functions definition "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "collapsed": true,
        "id": "PMmjWfxZu4wz"
      },
      "outputs": [],
      "source": [
        "def preprocess(sentence):\n",
        "    \"\"\"\n",
        "    Tokenizes, lowers, and stems\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer('english')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_list = stop_punctuation + stoplist\n",
        "\n",
        "    tokenized_mail = word_tokenize(sentence.lower())\n",
        "\n",
        "    pos_tagged_mail = pos_tag(tokenized_mail)\n",
        "\n",
        "    lemmatized_mail = [lemmatizer.lemmatize(word[0],get_wordnet_pos(word[1])) for word in pos_tagged_mail]\n",
        "    \n",
        "    return [stemmer.stem(word) for word in lemmatized_mail if word not in stop_list]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "prepro_hams = [preprocess(mail) for mail in hams]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['subject',\n",
              " 'ena',\n",
              " 'sale',\n",
              " 'hpl',\n",
              " 'updat',\n",
              " 'project',\n",
              " \"'\",\n",
              " 'status',\n",
              " 'base',\n",
              " 'new',\n",
              " 'report',\n",
              " 'scott',\n",
              " 'mill',\n",
              " 'ran',\n",
              " 'sitara',\n",
              " 'come',\n",
              " 'follow',\n",
              " 'counterparti',\n",
              " 'one',\n",
              " 'ena',\n",
              " 'sell',\n",
              " 'gas',\n",
              " 'hpl',\n",
              " \"'\",\n",
              " 'pipe',\n",
              " 'altrad',\n",
              " 'transact',\n",
              " 'l',\n",
              " 'l',\n",
              " 'c',\n",
              " 'gulf',\n",
              " 'gas',\n",
              " 'util',\n",
              " 'compani',\n",
              " 'brazoria',\n",
              " 'citi',\n",
              " 'panther',\n",
              " 'pipelin',\n",
              " 'inc',\n",
              " 'central',\n",
              " 'illinoi',\n",
              " 'light',\n",
              " 'compani',\n",
              " 'praxair',\n",
              " 'inc',\n",
              " 'central',\n",
              " 'power',\n",
              " 'light',\n",
              " 'compani',\n",
              " 'reliant',\n",
              " 'energi',\n",
              " '-',\n",
              " 'entex',\n",
              " 'ce',\n",
              " '-',\n",
              " 'equistar',\n",
              " 'chemic',\n",
              " 'lp',\n",
              " 'reliant',\n",
              " 'energi',\n",
              " '-',\n",
              " 'hl',\n",
              " '&',\n",
              " 'p',\n",
              " 'corpus',\n",
              " 'christi',\n",
              " 'gas',\n",
              " 'market',\n",
              " 'lp',\n",
              " 'southern',\n",
              " 'union',\n",
              " 'compani',\n",
              " '&',\n",
              " 'h',\n",
              " 'gas',\n",
              " 'compani',\n",
              " 'inc',\n",
              " 'texa',\n",
              " 'util',\n",
              " 'fuel',\n",
              " 'compani',\n",
              " 'duke',\n",
              " 'energi',\n",
              " 'field',\n",
              " 'servic',\n",
              " 'inc',\n",
              " 'txu',\n",
              " 'gas',\n",
              " 'distribut',\n",
              " 'entex',\n",
              " 'gas',\n",
              " 'market',\n",
              " 'compani',\n",
              " 'union',\n",
              " 'carbid',\n",
              " 'corpor',\n",
              " 'equistar',\n",
              " 'chemic',\n",
              " 'lp',\n",
              " 'unit',\n",
              " 'gas',\n",
              " 'transmiss',\n",
              " 'compani',\n",
              " 'inc',\n",
              " 'sinc',\n",
              " \"'\",\n",
              " 'sure',\n",
              " 'exact',\n",
              " 'get',\n",
              " 'enter',\n",
              " 'sitara',\n",
              " 'pat',\n",
              " 'clyne',\n",
              " 'suggest',\n",
              " 'check',\n",
              " 'daren',\n",
              " 'farmer',\n",
              " 'make',\n",
              " 'sure',\n",
              " \"'\",\n",
              " 'miss',\n",
              " 'someth',\n",
              " '(',\n",
              " ')',\n",
              " 'wait',\n",
              " 'respons',\n",
              " '/',\n",
              " 'mari',\n",
              " 'smith',\n",
              " 'begin',\n",
              " 'gather',\n",
              " 'contractu',\n",
              " 'volum',\n",
              " 'contract',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " 'forward',\n",
              " 'cheryl',\n",
              " 'dudley',\n",
              " '/',\n",
              " 'hou',\n",
              " '/',\n",
              " 'ect',\n",
              " '05',\n",
              " '/',\n",
              " '10',\n",
              " '/',\n",
              " '2000',\n",
              " '07',\n",
              " '56',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " '-',\n",
              " 'cheryl',\n",
              " 'king',\n",
              " '05',\n",
              " '/',\n",
              " '08',\n",
              " '/',\n",
              " '2000',\n",
              " '04',\n",
              " '11',\n",
              " 'pm',\n",
              " 'send',\n",
              " 'cheryl',\n",
              " 'dudley',\n",
              " 'daren',\n",
              " 'j',\n",
              " 'farmer',\n",
              " '/',\n",
              " 'hou',\n",
              " '/',\n",
              " 'ect',\n",
              " '@',\n",
              " 'ect',\n",
              " 'mari',\n",
              " 'smith',\n",
              " '/',\n",
              " 'hou',\n",
              " '/',\n",
              " 'ect',\n",
              " '@',\n",
              " 'ect',\n",
              " 'cc',\n",
              " 'subject',\n",
              " 'ena',\n",
              " 'sale',\n",
              " 'hpl',\n",
              " 'work',\n",
              " 'project',\n",
              " 'brenda',\n",
              " 'herod',\n",
              " '&',\n",
              " 'wonder',\n",
              " 'one',\n",
              " 'could',\n",
              " 'tell',\n",
              " \"'\",\n",
              " 'right',\n",
              " 'track',\n",
              " '&',\n",
              " 'get',\n",
              " 'everyth',\n",
              " 'look',\n",
              " 'tri',\n",
              " 'draft',\n",
              " 'long',\n",
              " '-',\n",
              " 'term',\n",
              " 'transport',\n",
              " '/',\n",
              " 'storag',\n",
              " 'agreement',\n",
              " 'ena',\n",
              " '&',\n",
              " 'hplc',\n",
              " 'allow',\n",
              " 'ena',\n",
              " 'move',\n",
              " 'gas',\n",
              " 'market',\n",
              " 'order',\n",
              " 'accomplish',\n",
              " 'need',\n",
              " 'know',\n",
              " 'sale',\n",
              " 'custom',\n",
              " 'ena',\n",
              " 'hpl',\n",
              " \"'\",\n",
              " 'pipe',\n",
              " 'scott',\n",
              " 'mill',\n",
              " 'run',\n",
              " 'report',\n",
              " 'sitara',\n",
              " 'show',\n",
              " 'ena',\n",
              " 'buy',\n",
              " '/',\n",
              " 'sell',\n",
              " 'activ',\n",
              " 'hpl',\n",
              " 'sinc',\n",
              " '7',\n",
              " '/',\n",
              " '99',\n",
              " 'elimin',\n",
              " 'buy',\n",
              " '&',\n",
              " 'desk',\n",
              " '-',\n",
              " '-',\n",
              " 'desk',\n",
              " 'deal',\n",
              " 'give',\n",
              " 'everyth',\n",
              " 'need',\n",
              " 'buy',\n",
              " '/',\n",
              " 'sell',\n",
              " 'deal',\n",
              " 'ena',\n",
              " 'hpl',\n",
              " \"'\",\n",
              " 'pipe',\n",
              " \"'\",\n",
              " 'show',\n",
              " 'sitara',\n",
              " 'someon',\n",
              " 'mention',\n",
              " 'someth',\n",
              " 'deal',\n",
              " 'hpl',\n",
              " 'transport',\n",
              " 'gas',\n",
              " \"'\",\n",
              " 'behalf',\n",
              " 'ena',\n",
              " 'sell',\n",
              " 'custom',\n",
              " 'spot',\n",
              " '-',\n",
              " '-',\n",
              " 'deal',\n",
              " 'like',\n",
              " 'happen',\n",
              " 'would',\n",
              " 'show',\n",
              " 'sitara',\n",
              " 'anyth',\n",
              " 'els',\n",
              " \"'\",\n",
              " 'miss',\n",
              " \"'\",\n",
              " 'real',\n",
              " 'familiar',\n",
              " 'deal',\n",
              " 'happen',\n",
              " 'nowaday',\n",
              " 'recept',\n",
              " 'idea',\n",
              " '/',\n",
              " 'suggest',\n",
              " '/',\n",
              " 'help',\n",
              " 'offer',\n",
              " 'thank',\n",
              " 'advanc']"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prepro_hams[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
